# LiveKit Agent - Complete Implementation Guide

This guide provides comprehensive documentation for the full LiveKit Agent implementation in React Native.

## ğŸ“ File Structure

```
app/(protected)/(drawer)/
  â””â”€â”€ livekit-agent.tsx           # Main agent screen

hooks/
  â”œâ”€â”€ useAgentState.ts             # Agent state management
  â””â”€â”€ useAgentTranscription.ts     # STT/TTS transcription

components/livekit-agent/
  â”œâ”€â”€ AgentVisualizer.tsx          # Animated agent avatar
  â”œâ”€â”€ AgentTranscript.tsx          # Conversation transcript
  â”œâ”€â”€ AgentControls.tsx            # Control panel
  â”œâ”€â”€ AgentMetrics.tsx             # Performance metrics
  â”œâ”€â”€ AgentSettings.tsx            # Configuration panel
  â””â”€â”€ FunctionCallDisplay.tsx      # Function call visualization
```

## ğŸ¯ Features Implemented

### 1. **Voice Activity Detection (VAD)**
- âœ… Real-time detection of user speech
- âœ… Automatic silence detection
- âœ… Push-to-talk mode
- âœ… Visual feedback for speaking state

### 2. **Speech-to-Text (STT)**
- âœ… Real-time transcription
- âœ… Interim results (streaming transcription)
- âœ… Final transcription segments
- âœ… User speech tracking
- âœ… Timestamp for each segment
- âœ… Transcription export

### 3. **Large Language Model (LLM)**
- âœ… Agent reasoning and response generation
- âœ… Context persistence
- âœ… Temperature control
- âœ… Max token configuration
- âœ… Turn management
- âœ… Conversation history

### 4. **Text-to-Speech (TTS)**
- âœ… Natural voice synthesis
- âœ… Multiple voice options
- âœ… Speaking speed control
- âœ… Real-time audio streaming
- âœ… Agent speech tracking

### 5. **Multimodal Capabilities**

#### Vision
- âœ… Vision capability detection
- âœ… Image/video processing ready
- ğŸ“ Note: Requires camera integration

#### Function Calling
- âœ… Function call tracking
- âœ… Arguments display
- âœ… Results visualization
- âœ… Real-time function execution monitoring
- âœ… Function call history

### 6. **Agent State Management**
- âœ… Comprehensive state tracking:
  - Disconnected
  - Connecting
  - Initializing
  - Listening
  - Thinking
  - Speaking
  - Idle
- âœ… State-based UI updates
- âœ… Visual state indicators
- âœ… Animated transitions

### 7. **Audio Visualization**
- âœ… Animated agent avatar
- âœ… Audio wave animation during speech
- âœ… Pulsing effects for listening
- âœ… Rotation animation for thinking
- âœ… State-based color coding

### 8. **Transcription Management**
- âœ… Full conversation transcript
- âœ… Separate user/agent tracking
- âœ… Current (interim) transcripts
- âœ… Final transcripts
- âœ… Transcript export functionality
- âœ… Segment timestamps
- âœ… Speaker identification

### 9. **Agent Controls**
- âœ… Microphone toggle
- âœ… Agent interruption
- âœ… Push-to-talk mode
- âœ… Auto-reconnect toggle
- âœ… Interruptible mode
- âœ… Clear transcript
- âœ… Disconnect

### 10. **Performance Metrics**

#### Latency Metrics
- âœ… STT latency tracking
- âœ… LLM response time
- âœ… TTS generation time
- âœ… Total round-trip latency

#### Session Metrics
- âœ… Session duration
- âœ… Conversation turns
- âœ… Interruption count
- âœ… Error tracking

#### Network Metrics
- âœ… Bytes received/sent
- âœ… Audio packet count
- âœ… Data packet count

### 11. **Configuration Settings**

#### Behavior Settings
- âœ… Auto interrupt
- âœ… End on silence
- âœ… Context persistence
- âœ… Voice activity detection

#### Audio Processing
- âœ… Echo cancellation
- âœ… Noise suppression
- âœ… Auto gain control

#### LLM Settings
- âœ… Temperature adjustment
- âœ… Max tokens configuration

#### TTS Settings
- âœ… Voice selection (4 voices)
- âœ… Speaking speed control

### 12. **UI/UX Features**

#### View Modes
- âœ… Conversation view (main)
- âœ… Transcript view (full history)
- âœ… Metrics view (performance)
- âœ… Settings view (configuration)

#### Visual Indicators
- âœ… Agent state indicator
- âœ… Capability badges (STT, LLM, TTS, Vision, Functions)
- âœ… Connection status
- âœ… Speaking/listening indicators
- âœ… Transcript counters

## ğŸ”§ Technical Implementation

### Agent State Hook

```typescript
import { useAgentState } from '@/hooks/useAgentState';

const {
  agentState,              // Current agent state
  agentParticipant,        // LiveKit participant
  capabilities,            // Agent capabilities array
  isAgentConnected,        // Connection status
  agentMetadata,           // Agent metadata
  setAgentState,           // Manual state update
} = useAgentState();
```

### Transcription Hook

```typescript
import { useAgentTranscription } from '@/hooks/useAgentTranscription';

const {
  transcript,              // Full transcript array
  userTranscript,          // User speech (current + final)
  agentTranscript,         // Agent speech (current + final)
  isTranscribing,          // STT active
  isSpeaking,              // TTS active
  addTranscript,           // Add custom segment
  clearTranscript,         // Clear all
} = useAgentTranscription();
```

### Agent Metadata Format

```typescript
{
  name: "AI Assistant",
  version: "1.0.0",
  capabilities: ["stt", "llm", "tts", "vision", "functions"],
  model: "gpt-4o",
  provider: "OpenAI"
}
```

### Data Channel Messages

The agent communicates via LiveKit data channels with these message types:

#### Transcription Messages
```json
{
  "type": "transcription",
  "speaker": "user" | "agent",
  "text": "Transcribed text",
  "is_final": true | false,
  "id": "unique-id",
  "timestamp": 1234567890
}
```

#### Function Call Messages
```json
{
  "type": "function_call",
  "function": "functionName",
  "arguments": { "arg1": "value" },
  "result": { "data": "result" },
  "timestamp": 1234567890
}
```

#### Agent Control Messages
```json
{
  "type": "interrupt",
  "timestamp": 1234567890
}
```

## ğŸš€ Usage

### Basic Setup

```typescript
import { ConnectionProvider } from '@/hooks/useConnection';
import LiveKitAgentScreen from './livekit-agent';

export default function App() {
  return (
    <ConnectionProvider>
      <LiveKitAgentScreen />
    </ConnectionProvider>
  );
}
```

### Navigation

```typescript
// Navigate to agent screen
router.push('/livekit-agent');
```

### Required Environment Variables

```bash
# LiveKit Server
EXPO_PUBLIC_LIVEKIT_URL=ws://your-server:7880

# Token Server
EXPO_PUBLIC_TOKEN_SERVER_URL=http://your-server:8008
```

## ğŸ¨ Customization

### Theme Colors

```typescript
const AGENT_COLORS = {
  listening: '#10B981',   // Green
  thinking: '#F59E0B',    // Orange
  speaking: '#3B82F6',    // Blue
  idle: '#6B7280',        // Gray
  error: '#EF4444',       // Red
};
```

### Agent Voices

Add more voices in `AgentSettings.tsx`:

```typescript
const voices = [
  { id: 'default', name: 'Default', description: 'Neutral voice' },
  { id: 'neural', name: 'Neural', description: 'Natural sounding' },
  { id: 'expressive', name: 'Expressive', description: 'Emotive voice' },
  { id: 'calm', name: 'Calm', description: 'Soothing tone' },
  // Add more voices here
];
```

## ğŸ“Š Backend Integration

### Python Agent Example

```python
from livekit import agents
from livekit.agents import STT, LLM, TTS

@agents.on_participant_connected
async def on_participant_connected(participant: agents.Participant):
    # Send agent metadata
    await participant.publish_data(json.dumps({
        "name": "AI Assistant",
        "version": "1.0.0",
        "capabilities": ["stt", "llm", "tts", "functions"],
        "model": "gpt-4o",
        "provider": "OpenAI"
    }))

# STT callback
async def on_speech(text: str, is_final: bool):
    await participant.publish_data(json.dumps({
        "type": "transcription",
        "speaker": "user",
        "text": text,
        "is_final": is_final,
        "timestamp": time.time() * 1000
    }))

# Agent response
async def on_agent_response(text: str):
    await participant.publish_data(json.dumps({
        "type": "transcription",
        "speaker": "agent",
        "text": text,
        "is_final": True,
        "timestamp": time.time() * 1000
    }))

# Function call
async def on_function_call(name: str, args: dict, result: dict):
    await participant.publish_data(json.dumps({
        "type": "function_call",
        "function": name,
        "arguments": args,
        "result": result,
        "timestamp": time.time() * 1000
    }))
```

### Node.js Agent Example

```typescript
import { Room } from 'livekit-server-sdk';

// Send agent metadata
await participant.publishData(JSON.stringify({
  name: "AI Assistant",
  version: "1.0.0",
  capabilities: ["stt", "llm", "tts", "functions"],
  model: "gpt-4o",
  provider: "OpenAI"
}));

// Send transcription
await participant.publishData(JSON.stringify({
  type: "transcription",
  speaker: "agent",
  text: agentResponse,
  is_final: true,
  timestamp: Date.now()
}));
```

## ğŸ§ª Testing

### Test Agent Connection

1. Start LiveKit server
2. Start agent backend
3. Launch React Native app
4. Navigate to agent screen
5. Agent should auto-connect and show "Ready" state

### Test Features

- **STT**: Speak and verify real-time transcription
- **LLM**: Check agent responses in transcript
- **TTS**: Verify agent speech audio
- **Interruption**: Speak while agent is talking
- **Function Calls**: Monitor function execution
- **Metrics**: Check latency and session stats

## ğŸ“± Platform Support

### iOS
- âœ… Audio session management
- âœ… Background audio
- âœ… Interruption handling
- âœ… Microphone permissions

### Android
- âœ… Audio focus management
- âœ… Background audio
- âœ… Microphone permissions

## ğŸ› Debugging

### Enable Debug Logging

```typescript
// In useAgentState.ts
console.log('Agent state changed:', agentState);
console.log('Agent capabilities:', capabilities);

// In useAgentTranscription.ts
console.log('New transcript:', segment);
console.log('Current user transcript:', userTranscript.current);
```

### Common Issues

1. **Agent not connecting**
   - Verify LiveKit server is running
   - Check token server URL
   - Confirm agent backend is active

2. **No transcription**
   - Check microphone permissions
   - Verify audio session is active
   - Confirm agent supports STT

3. **No function calls showing**
   - Verify data channel messages are being sent
   - Check message format matches expected schema

## ğŸ¯ Best Practices

1. **Always handle agent disconnection gracefully**
2. **Monitor latency metrics for performance**
3. **Test interruption scenarios thoroughly**
4. **Provide visual feedback for all agent states**
5. **Handle audio session lifecycle properly on iOS**
6. **Clear conversation history when needed**
7. **Export transcripts for user records**

## ğŸ”— Related Files

- `hooks/useConnection.tsx` - Connection management
- `services/livekit.ts` - LiveKit service
- `app/(protected)/(drawer)/voice-agent.tsx` - Simple voice agent
- `app/(protected)/(drawer)/livekit-full.tsx` - Full video features
- `server/agent.py` - Python agent backend

## ğŸ“š References

- [LiveKit Agents Documentation](https://docs.livekit.io/agents)
- [LiveKit React Native SDK](https://github.com/livekit/client-sdk-react-native)
- [LiveKit Components React](https://docs.livekit.io/frontends/start/frontends)
- [Voice Agent Guide](https://docs.livekit.io/agents/quickstart)

---

**Note**: This is a production-ready LiveKit agent implementation with all major features. Customize based on your specific use case and requirements.
